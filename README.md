ğŸŒŸ CustomAlign-RLHF

A lightweight, end-to-end RLHF pipeline for aligning language models using custom datasets.

CustomAlign-RLHF provides a simplified and modular implementation of Reinforcement Learning from Human Feedback (RLHF), enabling developers and researchers to align LLMs using their own domain-specific data.
The project covers the full pipeline: Supervised Fine-Tuning (SFT) â†’ Reward Modeling â†’ PPO Policy Optimization, similar to how modern aligned models like ChatGPT are trained.

ğŸš€ Features

Custom Dataset Support â€” Train LLMs using your own instruction and preference data.

Supervised Fine-Tuning (SFT) â€” Teach base models task-specific behavior.

Reward Model Training â€” Learn human preferences using pairwise comparison data.

PPO-based RL Optimization â€” Align model behavior with rewards using TRL.

Modular Codebase â€” SFT, reward modeling, and RL steps separated for clarity.

Lightweight & Accessible â€” Works on Kaggle/Colab with small models.

ğŸ› ï¸ Tech Stack

Python

HuggingFace Transformers

HuggingFace Datasets

TRL (for PPO training)

PyTorch

ğŸ“ Project Structure
CustomAlign-RLHF/
â”‚
â”œâ”€â”€ SFT/                   # Supervised fine-tuning scripts
â”œâ”€â”€ rewardModeling/        # Reward model training scripts
â”œâ”€â”€ policyOptimization/    # PPO alignment training
â”œâ”€â”€ assets/                # Images/diagrams used in README
â”œâ”€â”€ Judger/                # Preference pair evaluator (if included)
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

ğŸ“˜ RLHF Pipeline Overview
1ï¸âƒ£ Supervised Fine-Tuning (SFT)

Train the base model on your instruction-response dataset to give meaningful outputs.

2ï¸âƒ£ Reward Modeling

Train a model to score which response humans prefer using comparison pairs.

3ï¸âƒ£ PPO Policy Optimization

Use the reward model to guide the policy model in producing more aligned responses.

ğŸ“Š Example Use Cases

Domain-specific chatbots

Legal, medical, retail, or educational AI assistants

Safety-aligned LLM experiments

Learning + research on RLHF workflows

Fine-tuning small LLMs on custom instructions

â–¶ï¸ Getting Started

Clone the repository:

git clone https://github.com/deepak109patel/CustomAlign-RLHF.git
cd CustomAlign-RLHF


Install dependencies:

pip install -r requirements.txt


Run SFT training:

python SFT/train_sft.py


Train reward model:

python rewardModeling/train_reward_model.py


Run PPO alignment:

python policyOptimization/train_ppo.py

ğŸ“‘ Dataset Format
SFT Dataset
{
  "prompt": "Explain overfitting in ML.",
  "response": "Overfitting occurs when a model learns noise instead of general patterns..."
}

Reward Modeling Dataset
{
  "prompt": "Write a short poem.",
  "response_a": "...",
  "response_b": "...",
  "chosen": "b"
}

ğŸ¤ Contributing

Contributions, issues, and suggestions are welcome.
Feel free to open a PR!

ğŸ“„ License

This project is licensed under the MIT License.
